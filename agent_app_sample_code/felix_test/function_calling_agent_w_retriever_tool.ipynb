{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "grpcio-status 1.66.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.5 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qqqq -U databricks-agents databricks-vectorsearch databricks-sdk mlflow mlflow-skinny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Global configuration\n",
       "This notebook is meant to be changed once throughout the lifetime of your Agent.\n",
       "At a high-level it defines the Unity Catalog locations where the agent, and associated resources, will be stored. Ideally you shouldn't have to change the values in this notebook once you've started building your agent."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Agent configuration\n",
       "Important: These notebooks only work on Single User clusters running DBR/MLR 14.3+.\n",
       "To begin with, we simply need to configure the following:\n",
       "1. `AGENT_NAME`: The name of the Agent.  Used to name the app's Unity Catalog model and is prepended to the output Delta Tables + Vector Indexes\n",
       "2. `UC_CATALOG` & `UC_SCHEMA`: [Create a Unity Catalog](https://docs.databricks.com/en/data-governance/unity-catalog/create-catalogs.html#create-a-catalog) and a Schema where the output Delta Tables with the parsed/chunked documents and Vector Search indexes are stored\n",
       "3. `UC_MODEL_NAME`: Unity Catalog location to log and store the agent's model\n",
       "This notebook will also check that you are using a valid cluster type and all locations / resources exist. Any missing resources will be created."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Optional configuration\n",
       "- `MLFLOW_EXPERIMENT_NAME`: MLflow Experiment to track all experiments for this Agent.  Using the same experiment allows you to track runs across Notebooks and have unified lineage and governance for your Agent.\n",
       "- `EVALUATION_SET_FQN`: Delta Table where your evaluation set will be stored.  In the POC, we will seed the evaluation set with feedback you collect from your stakeholders."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--user info--\n",
      "user_name felixflory\n",
      "--agent--\n",
      "AGENT_NAME felix_cb_oct_agent\n",
      "UC_CATALOG felixflory\n",
      "UC_SCHEMA cookbook_felix_october_agent\n",
      "UC_MODEL_NAME felixflory.cookbook_felix_october_agent.felix_cb_oct_agent\n",
      "\n",
      "--evaluation config--\n",
      "EVALUATION_SET_FQN `felixflory`.`cookbook_felix_october_agent`.`felix_cb_oct_agent_evaluation_set`\n",
      "MLFLOW_EXPERIMENT_NAME /Users/felix.flory@databricks.com/felix_cb_oct_agent\n",
      "POC_DATA_PIPELINE_RUN_NAME data_pipeline_poc\n",
      "POC_CHAIN_RUN_NAME agent_poc\n",
      "PASS: UC catalog `felixflory` exists\n",
      "PASS: UC schema `felixflory.cookbook_felix_october_agent` exists\n"
     ]
    }
   ],
   "source": [
    "%run ../../agent_app_sample_code/00_global_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "##### AgentConfig\n",
       "`AgentConfig` is a configuration object that we use to communicate between our Agent notebook and the RAG chain that we log with mlflow, and also logged with mlflow. This configuration can be changed in conjunction with editing the chain file if parameters need to be edited."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run ../../agent_app_sample_code/agents/agent_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_config = RetrieverToolConfig(\n",
    "    vector_search_index=f\"{UC_CATALOG}.{UC_SCHEMA}.{AGENT_NAME}_chunked_docs_index\",\n",
    "    vector_search_schema=RetrieverSchemaConfig(\n",
    "        primary_key=\"chunk_id\",\n",
    "        chunk_text=\"content_chunked\",\n",
    "        document_uri=\"doc_uri\",\n",
    "        additional_metadata_columns=[],\n",
    "    ),\n",
    "    parameters=RetrieverParametersConfig(num_results=5, query_type=\"ann\"),\n",
    "    vector_search_threshold=0.1,\n",
    "    chunk_template=\"Passage text: {chunk_text}\\nPassage metadata: {metadata}\\n\\n\",\n",
    "    prompt_template=\"\"\"Use the following pieces of retrieved context to answer the question.\\nOnly use the passages from context that are relevant to the query to answer the question, ignore the irrelevant passages.  When responding, cite your source, referring to the passage by the columns in the passage's metadata.\\n\\nContext: {context}\"\"\",\n",
    "    tool_description_prompt=\"Search for documents that are relevant to a user's query about the [REPLACE WITH DESCRIPTION OF YOUR DOCS].\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_config = LLMConfig(\n",
    "    # https://docs.databricks.com/en/machine-learning/foundation-models/index.html\n",
    "    llm_endpoint_name=\"databricks-meta-llama-3-1-70b-instruct\",\n",
    "    # Define a template for the LLM prompt.  This is how the RAG chain combines the user's question and the retrieved context.\n",
    "    llm_system_prompt_template=(\n",
    "        \"\"\"You are a helpful assistant that answers questions by calling tools.  Provide responses ONLY based on the information from tools that are explictly specified to you.  If you do not have a relevant tool for a question, respond with 'Sorry, I'm not trained to answer that question'.\"\"\"\n",
    "    ),\n",
    "    # Parameters that control how the LLM responds.\n",
    "    llm_parameters=LLMParametersConfig(temperature=0.01, max_tokens=1500),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_config = AgentConfig(\n",
    "    retriever_tool=retriever_config,\n",
    "    llm_config=llm_config,\n",
    "    input_example={\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"What is RAG?\",\n",
    "            },\n",
    "        ]\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'retriever_tool': {'vector_search_index': 'felixflory.cookbook_felix_october_agent.felix_cb_oct_agent_chunked_docs_index',\n",
       "  'vector_search_schema': {'primary_key': 'chunk_id',\n",
       "   'chunk_text': 'content_chunked',\n",
       "   'document_uri': 'doc_uri',\n",
       "   'additional_metadata_columns': []},\n",
       "  'vector_search_threshold': 0.1,\n",
       "  'chunk_template': 'Passage text: {chunk_text}\\nPassage metadata: {metadata}\\n\\n',\n",
       "  'prompt_template': \"Use the following pieces of retrieved context to answer the question.\\nOnly use the passages from context that are relevant to the query to answer the question, ignore the irrelevant passages.  When responding, cite your source, referring to the passage by the columns in the passage's metadata.\\n\\nContext: {context}\",\n",
       "  'parameters': {'num_results': 5, 'query_type': 'ann'},\n",
       "  'tool_description_prompt': \"Search for documents that are relevant to a user's query about the [REPLACE WITH DESCRIPTION OF YOUR DOCS].\"},\n",
       " 'llm_config': {'llm_endpoint_name': 'databricks-meta-llama-3-1-70b-instruct',\n",
       "  'llm_system_prompt_template': \"You are a helpful assistant that answers questions by calling tools.  Provide responses ONLY based on the information from tools that are explictly specified to you.  If you do not have a relevant tool for a question, respond with 'Sorry, I'm not trained to answer that question'.\",\n",
       "  'llm_parameters': {'temperature': 0.01, 'max_tokens': 1500}},\n",
       " 'input_example': {'messages': [{'role': 'user', 'content': 'What is RAG?'}]}}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_config.dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Check if Vector Search Index exists"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Check LLM endpoint"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run ../../agent_app_sample_code/validators/validate_agent_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PASS: felixflory.cookbook_felix_october_agent.felix_cb_oct_agent_chunked_docs_index exists.\n",
      "PASS: Model serving endpoint databricks-meta-llama-3-1-70b-instruct is online & ready and supports task type /llm/v1/chat.  Details at: https://None/ml/endpoints/databricks-meta-llama-3-1-70b-instruct\n"
     ]
    }
   ],
   "source": [
    "validate_retriever_config(retriever_config)\n",
    "validate_llm_config(llm_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/10/15 08:14:25 INFO mlflow.tracking.fluent: Experiment with name '/Users/felix.flory@databricks.com/felix_cb_oct_agent' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:///Users/Felix.Flory/git/genai-cookbook/agent_app_sample_code/felix_test/mlruns/404873926099880626', creation_time=1728998065803, experiment_id='404873926099880626', last_update_time=1728998065803, lifecycle_stage='active', name='/Users/felix.flory@databricks.com/felix_cb_oct_agent', tags={}>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "mlflow.set_experiment(MLFLOW_EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code from agent_app_sample_code/agents/function_calling_agent_w_retriever_tool.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from typing import Any, Callable, Dict, List, Optional, Union\n",
    "import mlflow\n",
    "from dataclasses import asdict, dataclass\n",
    "import pandas as pd\n",
    "from mlflow.models import set_model, ModelConfig\n",
    "from mlflow.models.rag_signatures import StringResponse, ChatCompletionRequest, Message\n",
    "from mlflow.deployments import get_deploy_client\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from databricks.vector_search.client import VectorSearchClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Document:\n",
    "    page_content: str\n",
    "    metadata: Dict[str, str]\n",
    "    type: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorSearchRetriever:\n",
    "    \"\"\"\n",
    "    Class using Databricks Vector Search to retrieve relevant documents.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: Dict[str, Any]):\n",
    "        self.config = config\n",
    "        self.vector_search_client = VectorSearchClient(disable_notice=True)\n",
    "        self.vector_search_index = self.vector_search_client.get_index(\n",
    "            index_name=self.config.get(\"vector_search_index\")\n",
    "        )\n",
    "        vector_search_schema = self.config.get(\"vector_search_schema\")\n",
    "\n",
    "    def get_config(self) -> Dict[str, Any]:\n",
    "        return self.config\n",
    "\n",
    "    def get_tool_definition(self) -> Dict[str, Any]:\n",
    "        return {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"retrieve_documents\",\n",
    "                \"description\": self.config.get(\"tool_description_prompt\"),\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"query\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The query to find documents about.\",\n",
    "                        },\n",
    "                    },\n",
    "                    \"required\": [\"query\"],\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "\n",
    "    @mlflow.trace(span_type=\"TOOL\", name=\"VectorSearchRetriever\")\n",
    "    def __call__(self, query: str) -> str:\n",
    "        results = self.similarity_search(query)\n",
    "\n",
    "        context = \"\"\n",
    "        for result in results:\n",
    "            formatted_chunk = self.config.get(\"chunk_template\").format(\n",
    "                chunk_text=result.get(\"page_content\"),\n",
    "                metadata=json.dumps(result.get(\"metadata\")),\n",
    "            )\n",
    "            context += formatted_chunk\n",
    "\n",
    "        resulting_prompt = self.config.get(\"prompt_template\").format(context=context)\n",
    "\n",
    "        return resulting_prompt\n",
    "\n",
    "    @mlflow.trace(span_type=\"RETRIEVER\")\n",
    "    def similarity_search(\n",
    "        self, query: str, filters: Dict[Any, Any] = None\n",
    "    ) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Performs vector search to retrieve relevant chunks.\n",
    "\n",
    "        Args:\n",
    "            query: Search query.\n",
    "            filters: Optional filters to apply to the search, must follow the Databricks Vector Search filter spec (https://docs.databricks.com/en/generative-ai/create-query-vector-search.html#use-filters-on-queries)\n",
    "\n",
    "        Returns:\n",
    "            List of retrieved Documents.\n",
    "        \"\"\"\n",
    "\n",
    "        traced_search = mlflow.trace(\n",
    "            self.vector_search_index.similarity_search,\n",
    "            name=\"vector_search.similarity_search\",\n",
    "        )\n",
    "\n",
    "        # print(self.config)\n",
    "        columns = [\n",
    "            self.config.get(\"vector_search_schema\").get(\"primary_key\"),\n",
    "            self.config.get(\"vector_search_schema\").get(\"chunk_text\"),\n",
    "            self.config.get(\"vector_search_schema\").get(\"document_uri\"),\n",
    "        ] + self.config.get(\"vector_search_schema\").get(\"additional_metadata_columns\")\n",
    "\n",
    "        if filters is None:\n",
    "            results = traced_search(\n",
    "                query_text=query,\n",
    "                columns=columns,\n",
    "                **self.config.get(\"parameters\"),\n",
    "            )\n",
    "        else:\n",
    "            results = traced_search(\n",
    "                query_text=query,\n",
    "                filters=filters,\n",
    "                columns=columns,\n",
    "                **self.config.get(\"parameters\"),\n",
    "            )\n",
    "\n",
    "        vector_search_threshold = self.config.get(\"vector_search_threshold\")\n",
    "        documents = self.convert_vector_search_to_documents(\n",
    "            results, vector_search_threshold\n",
    "        )\n",
    "\n",
    "        return [asdict(doc) for doc in documents]\n",
    "\n",
    "    @mlflow.trace(span_type=\"PARSER\")\n",
    "    def convert_vector_search_to_documents(\n",
    "        self, vs_results, vector_search_threshold\n",
    "    ) -> List[Document]:\n",
    "        column_names = []\n",
    "        for column in vs_results[\"manifest\"][\"columns\"]:\n",
    "            column_names.append(column)\n",
    "\n",
    "        docs = []\n",
    "        if vs_results[\"result\"][\"row_count\"] > 0:\n",
    "            for item in vs_results[\"result\"][\"data_array\"]:\n",
    "                metadata = {}\n",
    "                score = item[-1]\n",
    "                if score >= vector_search_threshold:\n",
    "                    metadata[\"similarity_score\"] = score\n",
    "                    i = 0\n",
    "                    for field in item[0:-1]:\n",
    "                        metadata[column_names[i][\"name\"]] = field\n",
    "                        i = i + 1\n",
    "                    # put contents of the chunk into page_content\n",
    "                    page_content = metadata[\n",
    "                        self.config.get(\"vector_search_schema\").get(\"chunk_text\")\n",
    "                    ]\n",
    "                    del metadata[\n",
    "                        self.config.get(\"vector_search_schema\").get(\"chunk_text\")\n",
    "                    ]\n",
    "\n",
    "                    doc = Document(\n",
    "                        page_content=page_content, metadata=metadata, type=\"Document\"\n",
    "                    )  # , 9)\n",
    "                    docs.append(doc)\n",
    "\n",
    "        return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidInputException",
     "evalue": "Please specify either personal access token or service principal client ID and secret.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidInputException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 299\u001b[0m\n\u001b[1;32m    294\u001b[0m                 formatted_history\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAssistant: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmessage[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    296\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(formatted_history)\n\u001b[0;32m--> 299\u001b[0m set_model(\u001b[43mAgentWithRetriever\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[41], line 15\u001b[0m, in \u001b[0;36mAgentWithRetriever.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_serving_client \u001b[38;5;241m=\u001b[39m get_deploy_client(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatabricks\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Init the Retriever tool\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretriever_tool \u001b[38;5;241m=\u001b[39m \u001b[43mVectorSearchRetriever\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mretriever_tool\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Configure the Review App to use the Retriever's schema\u001b[39;00m\n\u001b[1;32m     18\u001b[0m vector_search_schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mretriever_tool\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvector_search_schema\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     20\u001b[0m )\n",
      "Cell \u001b[0;32mIn[37], line 8\u001b[0m, in \u001b[0;36mVectorSearchRetriever.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config: Dict[\u001b[38;5;28mstr\u001b[39m, Any]):\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig \u001b[38;5;241m=\u001b[39m config\n\u001b[0;32m----> 8\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvector_search_client \u001b[38;5;241m=\u001b[39m \u001b[43mVectorSearchClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdisable_notice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvector_search_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvector_search_client\u001b[38;5;241m.\u001b[39mget_index(\n\u001b[1;32m     10\u001b[0m         index_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvector_search_index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m     )\n\u001b[1;32m     12\u001b[0m     vector_search_schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvector_search_schema\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/git/genai-cookbook/.conda/lib/python3.11/site-packages/databricks/vector_search/client.py:61\u001b[0m, in \u001b[0;36mVectorSearchClient.__init__\u001b[0;34m(self, workspace_url, personal_access_token, service_principal_client_id, service_principal_client_secret, disable_notice)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_control_plane_oauth_token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_control_plane_oauth_token_expiry_ts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdisable_notice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_notice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git/genai-cookbook/.conda/lib/python3.11/site-packages/databricks/vector_search/client.py:67\u001b[0m, in \u001b[0;36mVectorSearchClient.validate\u001b[0;34m(self, disable_notice)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalidate\u001b[39m(\u001b[38;5;28mself\u001b[39m, disable_notice\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpersonal_access_token \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m     65\u001b[0m             (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mservice_principal_client_id \u001b[38;5;129;01mand\u001b[39;00m \n\u001b[1;32m     66\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mservice_principal_client_secret)):\n\u001b[0;32m---> 67\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidInputException(\n\u001b[1;32m     68\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease specify either personal access token or service principal client ID and secret.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     69\u001b[0m         )\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mservice_principal_client_id \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m     71\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mservice_principal_client_secret \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m     72\u001b[0m             \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkspace_url):\n\u001b[1;32m     73\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidInputException(\n\u001b[1;32m     74\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mService Principal auth flow requires workspace url\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     75\u001b[0m         )\n",
      "\u001b[0;31mInvalidInputException\u001b[0m: Please specify either personal access token or service principal client ID and secret."
     ]
    }
   ],
   "source": [
    "class AgentWithRetriever(mlflow.pyfunc.PythonModel):\n",
    "    \"\"\"\n",
    "    Class representing an Agent that includes a Retriever tool\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.config = mlflow.models.ModelConfig(\n",
    "            development_config=\"../../agent_app_sample_code/agents/generated_configs/agent.yaml\"\n",
    "        )\n",
    "\n",
    "        # Load the LLM\n",
    "        self.model_serving_client = get_deploy_client(\"databricks\")\n",
    "\n",
    "        # Init the Retriever tool\n",
    "        self.retriever_tool = VectorSearchRetriever(self.config.get(\"retriever_tool\"))\n",
    "\n",
    "        # Configure the Review App to use the Retriever's schema\n",
    "        vector_search_schema = self.config.get(\"retriever_tool\").get(\n",
    "            \"vector_search_schema\"\n",
    "        )\n",
    "        mlflow.models.set_retriever_schema(\n",
    "            primary_key=vector_search_schema.get(\"primary_key\"),\n",
    "            text_column=vector_search_schema.get(\"chunk_text\"),\n",
    "            doc_uri=vector_search_schema.get(\"doc_uri\"),\n",
    "        )\n",
    "\n",
    "        self.tool_functions = {\n",
    "            \"retrieve_documents\": self.retriever_tool,\n",
    "        }\n",
    "\n",
    "        # Internal representation of the chat history.  As the Agent iteratively selects/executes tools, the history will be stored here.  Since the Agent is stateless, this variable must be populated on each invocation of `predict(...)`.\n",
    "        self.chat_history = None\n",
    "\n",
    "    @mlflow.trace(name=\"chain\", span_type=\"CHAIN\")\n",
    "    def predict(\n",
    "        self,\n",
    "        context: Any = None,\n",
    "        model_input: Union[ChatCompletionRequest, Dict, pd.DataFrame] = None,\n",
    "        params: Any = None,\n",
    "    ) -> StringResponse:\n",
    "        ##############################################################################\n",
    "        # Extract `messages` key from the `model_input`\n",
    "        messages = self.get_messages_array(model_input)\n",
    "\n",
    "        ##############################################################################\n",
    "        # Parse `messages` array into the user's query & the chat history\n",
    "        with mlflow.start_span(name=\"parse_input\", span_type=\"PARSER\") as span:\n",
    "            span.set_inputs({\"messages\": messages})\n",
    "            user_query = self.extract_user_query_string(messages)\n",
    "            # Save the history inside the Agent's internal state\n",
    "            self.chat_history = self.extract_chat_history(messages)\n",
    "            span.set_outputs(\n",
    "                {\"user_query\": user_query, \"chat_history\": self.chat_history}\n",
    "            )\n",
    "\n",
    "        ##############################################################################\n",
    "        # Generate Answer\n",
    "        system_prompt = self.config.get(\"llm_config\").get(\"llm_system_prompt_template\")\n",
    "\n",
    "        # Add the previous history\n",
    "        messages = (\n",
    "            [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "            + self.chat_history  # append chat history for multi turn\n",
    "            + [{\"role\": \"user\", \"content\": user_query}]\n",
    "        )\n",
    "\n",
    "        # Call the LLM to recursively calls tools and eventually deliver a generation to send back to the user\n",
    "        (\n",
    "            model_response,\n",
    "            messages_log_with_tool_calls,\n",
    "        ) = self.recursively_call_and_run_tools(messages=messages)\n",
    "\n",
    "        # If your front end keeps of converastion history and automatically appends the bot's response to the messages history, remove this line.\n",
    "        messages_log_with_tool_calls.append(model_response.choices[0][\"message\"])\n",
    "\n",
    "        # remove the system prompt - this should not be exposed to the Agent caller\n",
    "        messages_log_with_tool_calls = messages_log_with_tool_calls[1:]\n",
    "\n",
    "        return {\n",
    "            \"content\": model_response.choices[0][\"message\"][\"content\"],\n",
    "            # this should be returned back to the Review App (or any other front end app) and stored there so it can be passed back to this stateless agent with the next turns of converastion.\n",
    "            \"messages\": messages_log_with_tool_calls,\n",
    "        }\n",
    "\n",
    "    @mlflow.trace(span_type=\"TOOL\")\n",
    "    def retrieve_documents(self, query, doc_name_filter) -> Dict:\n",
    "        # Rewrite the query e.g., \"what is it?\" to \"what is [topic from previous question]\".  Test the chain with and without this - some function calling models automatically handle the query rewriting e.g., when they call the tool, they rewrite the query\n",
    "        vs_query = query\n",
    "        if len(self.chat_history) > 0:\n",
    "            vs_query = self.query_rewrite(query, self.chat_history)\n",
    "        else:\n",
    "            vs_query = query\n",
    "\n",
    "        # print(doc_name_filter)\n",
    "        results = self.customer_notes_retriever.similarity_search(vs_query)\n",
    "\n",
    "        context = \"\"\n",
    "        for result in results:\n",
    "            context += \"Document: \" + json.dumps(result) + \"\\n\"\n",
    "\n",
    "        resulting_prompt = (\n",
    "            self.config.get(\"retriever_tool\")\n",
    "            .get(\"prompt_template\")\n",
    "            .format(context=context)\n",
    "        )\n",
    "\n",
    "        return resulting_prompt  # json.dumps(results, default=str)\n",
    "\n",
    "    @mlflow.trace(span_type=\"PARSER\")\n",
    "    def query_rewrite(self, query, chat_history) -> str:\n",
    "        ############\n",
    "        # Prompt Template for query rewriting to allow converastion history to work - this will translate a query such as \"how does it work?\" after a question such as \"what is spark?\" to \"how does spark work?\".\n",
    "        ############\n",
    "        query_rewrite_template = \"\"\"Based on the chat history below, we want you to generate a query for an external data source to retrieve relevant documents so that we can better answer the question. The query should be in natural language. The external data source uses similarity search to search for relevant documents in a vector space. So the query should be similar to the relevant documents semantically. Answer with only the query. Do not add explanation.\n",
    "\n",
    "        Chat history: {chat_history}\n",
    "\n",
    "        Question: {question}\"\"\"\n",
    "\n",
    "        chat_history_formatted = self.format_chat_history(chat_history)\n",
    "\n",
    "        prompt = query_rewrite_template.format(\n",
    "            question=query, chat_history=chat_history_formatted\n",
    "        )\n",
    "\n",
    "        model_response = self.chat_completion(\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        if len(model_response.choices) > 0:\n",
    "            return model_response.choices[0].message.content\n",
    "        else:\n",
    "            # if no generation, return the original query\n",
    "            return query\n",
    "\n",
    "    @mlflow.trace(span_type=\"AGENT\")\n",
    "    def recursively_call_and_run_tools(self, max_iter=10, **kwargs):\n",
    "        messages = kwargs[\"messages\"]\n",
    "        del kwargs[\"messages\"]\n",
    "        i = 0\n",
    "        while i < max_iter:\n",
    "            response = self.chat_completion(messages=messages, tools=True)\n",
    "            assistant_message = response.choices[0][\"message\"]\n",
    "            tool_calls = assistant_message.get(\"tool_calls\")\n",
    "            if tool_calls is None:\n",
    "                # the tool execution finished, and we have a generation\n",
    "                return (response, messages)\n",
    "            tool_messages = []\n",
    "            for tool_call in tool_calls:\n",
    "                function = tool_call[\"function\"]\n",
    "                args = json.loads(function[\"arguments\"])\n",
    "                result = self.execute_function(function[\"name\"], args)\n",
    "                tool_message = {\n",
    "                    \"role\": \"tool\",\n",
    "                    \"tool_call_id\": tool_call[\"id\"],\n",
    "                    \"content\": result,\n",
    "                }\n",
    "                tool_messages.append(tool_message)\n",
    "            assistant_message_dict = assistant_message.copy()\n",
    "            del assistant_message_dict[\"content\"]\n",
    "            messages = (\n",
    "                messages\n",
    "                + [\n",
    "                    assistant_message_dict,\n",
    "                ]\n",
    "                + tool_messages\n",
    "            )\n",
    "        raise \"ERROR: max iter reached\"\n",
    "\n",
    "    @mlflow.trace(span_type=\"FUNCTION\")\n",
    "    def execute_function(self, function_name, args):\n",
    "        the_function = self.tool_functions.get(function_name)\n",
    "        result = the_function(**args)\n",
    "        return result\n",
    "\n",
    "    def chat_completion(self, messages: List[Dict[str, str]], tools: bool = False):\n",
    "        endpoint_name = self.config.get(\"llm_config\").get(\"llm_endpoint_name\")\n",
    "        llm_options = self.config.get(\"llm_config\").get(\"llm_parameters\")\n",
    "\n",
    "        # Trace the call to Model Serving\n",
    "        traced_create = mlflow.trace(\n",
    "            self.model_serving_client.predict,\n",
    "            name=\"chat_completions_api\",\n",
    "            span_type=\"CHAT_MODEL\",\n",
    "        )\n",
    "\n",
    "        if tools:\n",
    "            # Get all tools\n",
    "            tools = [self.retriever_tool.get_tool_definition()]\n",
    "\n",
    "            inputs = {\n",
    "                \"messages\": messages,\n",
    "                \"tools\": tools,\n",
    "                **llm_options,\n",
    "            }\n",
    "        else:\n",
    "            inputs = {\n",
    "                \"messages\": messages,\n",
    "                **llm_options,\n",
    "            }\n",
    "\n",
    "        # Use the traced_create to make the prediction\n",
    "        return traced_create(\n",
    "            endpoint=endpoint_name,\n",
    "            inputs=inputs,\n",
    "        )\n",
    "\n",
    "    @mlflow.trace(span_type=\"PARSER\")\n",
    "    def get_messages_array(\n",
    "        self, model_input: Union[ChatCompletionRequest, Dict, pd.DataFrame]\n",
    "    ) -> List[Dict[str, str]]:\n",
    "        if type(model_input) == ChatCompletionRequest:\n",
    "            return model_input.messages\n",
    "        elif type(model_input) == dict:\n",
    "            return model_input.get(\"messages\")\n",
    "        elif type(model_input) == pd.DataFrame:\n",
    "            return model_input.iloc[0].to_dict().get(\"messages\")\n",
    "\n",
    "    @mlflow.trace(span_type=\"PARSER\")\n",
    "    def extract_user_query_string(\n",
    "        self, chat_messages_array: List[Dict[str, str]]\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Extracts user query string from the chat messages array.\n",
    "\n",
    "        Args:\n",
    "            chat_messages_array: Array of chat messages.\n",
    "\n",
    "        Returns:\n",
    "            User query string.\n",
    "        \"\"\"\n",
    "\n",
    "        if isinstance(chat_messages_array, pd.Series):\n",
    "            chat_messages_array = chat_messages_array.tolist()\n",
    "\n",
    "        if isinstance(chat_messages_array[-1], dict):\n",
    "            return chat_messages_array[-1][\"content\"]\n",
    "        elif isinstance(chat_messages_array[-1], Message):\n",
    "            return chat_messages_array[-1].content\n",
    "        else:\n",
    "            return chat_messages_array[-1]\n",
    "\n",
    "    @mlflow.trace(span_type=\"PARSER\")\n",
    "    def extract_chat_history(\n",
    "        self, chat_messages_array: List[Dict[str, str]]\n",
    "    ) -> List[Dict[str, str]]:\n",
    "        \"\"\"\n",
    "        Extracts the chat history from the chat messages array.\n",
    "\n",
    "        Args:\n",
    "            chat_messages_array: Array of Dictionary representing each chat messages.\n",
    "\n",
    "        Returns:\n",
    "            The chat history.\n",
    "        \"\"\"\n",
    "        # Convert DataFrame to dict\n",
    "        if isinstance(chat_messages_array, pd.Series):\n",
    "            chat_messages_array = chat_messages_array.tolist()\n",
    "\n",
    "        # Dictionary, return as is\n",
    "        if isinstance(chat_messages_array[0], dict):\n",
    "            return chat_messages_array[:-1]  # return all messages except the last one\n",
    "        # MLflow Message, convert to Dictionary\n",
    "        elif isinstance(chat_messages_array[0], Message):\n",
    "            new_array = []\n",
    "            for message in chat_messages_array[:-1]:\n",
    "                new_array.append(asdict(message))\n",
    "            return new_array\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"chat_messages_array is not an Array of Dictionary, Pandas DataFrame, or array of MLflow Message.\"\n",
    "            )\n",
    "\n",
    "    @mlflow.trace(span_type=\"PARSER\")\n",
    "    def format_chat_history(self, chat_history: List[Dict[str, str]]) -> str:\n",
    "        \"\"\"\n",
    "        Formats the chat history into a string.\n",
    "\n",
    "        Args:\n",
    "            chat_history: List of chat messages.\n",
    "\n",
    "        Returns:\n",
    "            Formatted chat history string.\n",
    "        \"\"\"\n",
    "        if not chat_history:\n",
    "            return \"\"\n",
    "\n",
    "        formatted_history = []\n",
    "        for message in chat_history:\n",
    "            if message[\"role\"] == \"user\":\n",
    "                formatted_history.append(f\"User: {message['content']}\")\n",
    "\n",
    "            # this logic ignores assistant messages that are just about tool calling and have no user facing content\n",
    "            elif message[\"role\"] == \"assistant\" and message.get(\"content\"):\n",
    "                formatted_history.append(f\"Assistant: {message['content']}\")\n",
    "\n",
    "        return \"\\n\".join(formatted_history)\n",
    "\n",
    "\n",
    "set_model(AgentWithRetriever())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
